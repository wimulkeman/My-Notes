Entropy
=======

Entropy describes the value of information.

**Low entropy** means that the contents of the message is predictable.

**High entropy** means that the contents of the message is highly unpredictable.

With calculating the entropy of information, the number of bits required to transmit the information van be calculated.
The entropy can be calculated by number of bits average needed to transmit the information.

When there are 4 possible states, the calculation is as following.

Change = 1/4 = 1/2<sup>2</sup> = 2<sup>-2</sup>

This equals the formula, where p = probability of a something happening (1/4).
 
number of bits needed on average = p x log p

Resources:

* YouTube - [Entropy in compression - Computerphile](https://www.youtube.com/watch?annotation_id=annotation_818567&feature=iv&src_vid=Lto-ajuqW3w&v=M5c_RFKVkko)
* Wikipedia - [Entropy (Information theory)](https://en.wikipedia.org/wiki/Entropy_(information_theory))